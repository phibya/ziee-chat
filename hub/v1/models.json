{
  "hub_version": "v1",
  "schema_version": 2,
  "models": [
    {
      "id": "llama-3-8b-instruct",
      "name": "Llama 3 8B Instruct",
      "alias": "Llama 3 8B",
      "description": "Meta's Llama 3 8B instruction-tuned model",
      "repository_url": "https://huggingface.co",
      "repository_path": "meta-llama/Meta-Llama-3-8B-Instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true
      },
      "size_gb": 15.8,
      "tags": [
        "instruct",
        "chat",
        "code",
        "reasoning"
      ],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 8192
      },
      "public": false,
      "popularity_score": 0.95,
      "license": "Meta Custom License",
      "quantization_options": [
        "fp16",
        "q4_0",
        "q8_0"
      ],
      "context_length": 8192,
      "language_support": [
        "en",
        "es",
        "fr",
        "de",
        "it"
      ],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "plain",
        "device_type": "auto",
        "max_seqs": 16,
        "max_sequence_len": 8192,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "llama"
      }
    },
    {
      "id": "llama-3-1-8b-instruct",
      "name": "Llama 3.1 8B Instruct",
      "alias": "Llama 3.1 8B",
      "description": "Meta's latest Llama 3.1 8B instruction-tuned model with improved capabilities",
      "repository_url": "https://huggingface.co",
      "repository_path": "meta-llama/Llama-3.1-8B-Instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true
      },
      "size_gb": 16.1,
      "tags": [
        "instruct",
        "chat",
        "code",
        "reasoning",
        "latest"
      ],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 8192
      },
      "public": false,
      "popularity_score": 0.97,
      "license": "Meta Custom License",
      "quantization_options": [
        "fp16",
        "q4_0",
        "q8_0"
      ],
      "context_length": 8192,
      "language_support": [
        "en",
        "es",
        "fr",
        "de",
        "it",
        "pt",
        "nl",
        "pl"
      ],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "plain",
        "device_type": "auto",
        "max_seqs": 16,
        "max_sequence_len": 8192,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "llama"
      }
    },
    {
      "id": "phi-3-mini-4k-instruct",
      "name": "Phi-3 Mini 4K Instruct",
      "alias": "Phi-3 Mini",
      "description": "Microsoft's small but powerful instruction-tuned model",
      "repository_url": "https://huggingface.co",
      "repository_path": "microsoft/Phi-3-mini-4k-instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true
      },
      "size_gb": 7.6,
      "tags": [
        "instruct",
        "chat",
        "small",
        "efficient"
      ],
      "recommended_parameters": {
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 50,
        "repeat_penalty": 1.05,
        "context_size": 4096
      },
      "public": true,
      "popularity_score": 0.87,
      "license": "MIT",
      "quantization_options": [
        "fp16",
        "q4_0",
        "q8_0"
      ],
      "context_length": 4096,
      "language_support": [
        "en"
      ],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "plain",
        "device_type": "auto",
        "max_seqs": 8,
        "max_sequence_len": 4096,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "phi"
      }
    },
    {
      "id": "llava-v1.6-vicuna-7b",
      "name": "LLaVA v1.6 Vicuna 7B",
      "alias": "LLaVA Vicuna 7B",
      "description": "Large Language and Vision Assistant with Vicuna 7B base",
      "repository_url": "https://huggingface.co",
      "repository_path": "liuhaotian/llava-v1.6-vicuna-7b",
      "main_filename": "pytorch_model.bin",
      "file_format": "pytorch",
      "capabilities": {
        "vision": true,
        "audio": false,
        "tools": false,
        "code_interpreter": false
      },
      "size_gb": 13.4,
      "tags": [
        "vision",
        "multimodal",
        "image-understanding"
      ],
      "recommended_parameters": {
        "temperature": 0.5,
        "top_p": 0.9,
        "top_k": 30,
        "repeat_penalty": 1.0,
        "context_size": 2048
      },
      "public": true,
      "popularity_score": 0.78,
      "license": "Apache 2.0",
      "quantization_options": [
        "fp16",
        "q4_0"
      ],
      "context_length": 2048,
      "language_support": [
        "en"
      ],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "vision-plain",
        "device_type": "auto",
        "max_seqs": 4,
        "max_sequence_len": 2048,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "llava"
      }
    },
    {
      "id": "qwen2.5-vl-3b-instruct",
      "name": "Qwen2.5-VL 3B Instruct",
      "alias": "Qwen2.5-VL 3B",
      "description": "Qwen2.5-VL 3B instruction-tuned vision-language model",
      "repository_url": "https://huggingface.co",
      "repository_path": "Qwen/Qwen2.5-VL-3B-Instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": true,
        "audio": false,
        "tools": true,
        "code_interpreter": true
      },
      "size_gb": 6.2,
      "tags": [
        "vision",
        "multimodal",
        "instruct",
        "chat",
        "image-understanding"
      ],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.8,
        "top_k": 40,
        "repeat_penalty": 1.05,
        "context_size": 32768
      },
      "public": true,
      "popularity_score": 0.85,
      "license": "Apache 2.0",
      "quantization_options": [
        "fp16",
        "q4_0",
        "q8_0"
      ],
      "context_length": 32768,
      "language_support": [
        "en",
        "zh",
        "ja",
        "ko",
        "vi",
        "th",
        "ar",
        "es",
        "fr",
        "ru",
        "de"
      ],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "vision-plain",
        "device_type": "auto",
        "max_seqs": 8,
        "max_sequence_len": 32768,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "qwen2"
      }
    },
    {
      "id": "tinyllama-1.1b-chat-gguf",
      "name": "TinyLlama 1.1B Chat GGUF",
      "alias": "TinyLlama Chat",
      "description": "Small, efficient chat model in GGUF format - perfect for testing LlamaCpp engine",
      "repository_url": "https://huggingface.co",
      "repository_path": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
      "main_filename": "tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
      "file_format": "gguf",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": false,
        "code_interpreter": true
      },
      "size_gb": 0.7,
      "tags": [
        "chat",
        "small",
        "efficient",
        "gguf",
        "llamacpp",
        "testing"
      ],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 2048
      },
      "public": true,
      "popularity_score": 0.75,
      "license": "Apache 2.0",
      "quantization_options": [
        "q4_0",
        "q4_1",
        "q5_0",
        "q5_1",
        "q8_0",
        "fp16"
      ],
      "context_length": 2048,
      "language_support": [
        "en"
      ],
      "recommended_engine": "llamacpp",
      "recommended_engine_settings": {
        "device_type": "auto",
        "ctx_size": 2048,
        "batch_size": 512,
        "parallel": 1,
        "cont_batching": true,
        "flash_attn": false
      }
    },
    {
      "id": "llama-3-2-3b-instruct-gguf",
      "name": "Llama 3.2 3B Instruct GGUF",
      "alias": "Llama 3.2 3B",
      "description": "Meta's Llama 3.2 3B instruction-tuned model in GGUF format, optimized for efficiency",
      "repository_url": "https://huggingface.co",
      "repository_path": "bartowski/Llama-3.2-3B-Instruct-GGUF",
      "main_filename": "Llama-3.2-3B-Instruct-Q5_K_S.gguf",
      "file_format": "gguf",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true
      },
      "size_gb": 1.9,
      "tags": [
        "instruct",
        "chat",
        "code",
        "reasoning",
        "gguf",
        "efficient",
        "small"
      ],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 8192
      },
      "public": true,
      "popularity_score": 0.88,
      "license": "Meta Custom License",
      "quantization_options": [
        "q4_0",
        "q4_k_m",
        "q5_0",
        "q5_k_m",
        "q6_k",
        "q8_0",
        "fp16"
      ],
      "context_length": 8192,
      "language_support": [
        "en",
        "es",
        "fr",
        "de",
        "it",
        "pt",
        "nl"
      ],
      "recommended_engine": "llamacpp",
      "recommended_engine_settings": {
        "device_type": "auto",
        "ctx_size": 8192,
        "batch_size": 512,
        "parallel": 1,
        "cont_batching": true,
        "flash_attn": false
      }
    }
  ]
}
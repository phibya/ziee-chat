{
  "hub_version": "v1",
  "schema_version": 2,
  "models": [
    {
      "id": "llama-3-8b-instruct",
      "name": "Llama 3 8B Instruct",
      "display_name": "Llama 3 8B",
      "description": "Meta's Llama 3 8B instruction-tuned model",
      "repository_url": "https://huggingface.co",
      "repository_path": "meta-llama/Meta-Llama-3-8B-Instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true,
        "chat": true,
        "text_embedding": false,
        "image_generator": false
      },
      "size_gb": 15.8,
      "tags": ["instruct", "chat", "code", "reasoning"],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 8192
      },
      "public": false,
      "popularity_score": 0.95,
      "license": "Meta Custom License",
      "context_length": 8192,
      "language_support": ["en", "es", "fr", "de", "it"],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "plain",
        "device_type": "auto",
        "max_seqs": 16,
        "max_sequence_len": 8192,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "llama"
      }
    },
    {
      "id": "llama-3-1-8b-instruct",
      "name": "Llama 3.1 8B Instruct",
      "display_name": "Llama 3.1 8B",
      "description": "Meta's latest Llama 3.1 8B instruction-tuned model with improved capabilities",
      "repository_url": "https://huggingface.co",
      "repository_path": "meta-llama/Llama-3.1-8B-Instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true,
        "chat": true,
        "text_embedding": false,
        "image_generator": false
      },
      "size_gb": 16.1,
      "tags": ["instruct", "chat", "code", "reasoning", "latest"],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 8192
      },
      "public": false,
      "popularity_score": 0.97,
      "license": "Meta Custom License",
      "context_length": 8192,
      "language_support": ["en", "es", "fr", "de", "it", "pt", "nl", "pl"],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "plain",
        "device_type": "auto",
        "max_seqs": 16,
        "max_sequence_len": 8192,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "llama"
      }
    },
    {
      "id": "phi-3-mini-4k-instruct",
      "name": "Phi-3 Mini 4K Instruct",
      "display_name": "Phi-3 Mini",
      "description": "Microsoft's small but powerful instruction-tuned model",
      "repository_url": "https://huggingface.co",
      "repository_path": "microsoft/Phi-3-mini-4k-instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true,
        "chat": true,
        "text_embedding": false,
        "image_generator": false
      },
      "size_gb": 7.6,
      "tags": ["instruct", "chat", "small", "efficient"],
      "recommended_parameters": {
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 50,
        "repeat_penalty": 1.05,
        "context_size": 4096
      },
      "public": true,
      "popularity_score": 0.87,
      "license": "MIT",
      "context_length": 4096,
      "language_support": ["en"],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "plain",
        "device_type": "auto",
        "max_seqs": 8,
        "max_sequence_len": 4096,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "phi"
      }
    },
    {
      "id": "llava-v1.6-vicuna-7b",
      "name": "LLaVA v1.6 Vicuna 7B",
      "display_name": "LLaVA Vicuna 7B",
      "description": "Large Language and Vision Assistant with Vicuna 7B base",
      "repository_url": "https://huggingface.co",
      "repository_path": "liuhaotian/llava-v1.6-vicuna-7b",
      "main_filename": "pytorch_model.bin",
      "file_format": "pytorch",
      "capabilities": {
        "vision": true,
        "audio": false,
        "tools": false,
        "code_interpreter": false,
        "chat": true,
        "text_embedding": false,
        "image_generator": false
      },
      "size_gb": 13.4,
      "tags": ["vision", "multimodal", "image-understanding"],
      "recommended_parameters": {
        "temperature": 0.5,
        "top_p": 0.9,
        "top_k": 30,
        "repeat_penalty": 1.0,
        "context_size": 2048
      },
      "public": true,
      "popularity_score": 0.78,
      "license": "Apache 2.0",
      "context_length": 2048,
      "language_support": ["en"],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "vision-plain",
        "device_type": "auto",
        "max_seqs": 4,
        "max_sequence_len": 2048,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "llava"
      }
    },
    {
      "id": "qwen2.5-vl-3b-instruct",
      "name": "Qwen2.5-VL 3B Instruct",
      "display_name": "Qwen2.5-VL 3B",
      "description": "Qwen2.5-VL 3B instruction-tuned vision-language model",
      "repository_url": "https://huggingface.co",
      "repository_path": "Qwen/Qwen2.5-VL-3B-Instruct",
      "main_filename": "model.safetensors",
      "file_format": "safetensors",
      "capabilities": {
        "vision": true,
        "audio": false,
        "tools": true,
        "code_interpreter": true,
        "chat": true,
        "text_embedding": false,
        "image_generator": false
      },
      "size_gb": 6.2,
      "tags": [
        "vision",
        "multimodal",
        "instruct",
        "chat",
        "image-understanding"
      ],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.8,
        "top_k": 40,
        "repeat_penalty": 1.05,
        "context_size": 32768
      },
      "public": true,
      "popularity_score": 0.85,
      "license": "Apache 2.0",
      "context_length": 32768,
      "language_support": [
        "en",
        "zh",
        "ja",
        "ko",
        "vi",
        "th",
        "ar",
        "es",
        "fr",
        "ru",
        "de"
      ],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "command": "vision-plain",
        "device_type": "auto",
        "max_seqs": 8,
        "max_sequence_len": 32768,
        "chat_template": true,
        "tokenizer_json": null,
        "arch": "qwen2"
      }
    },
    {
      "id": "tinyllama-1.1b-chat-gguf",
      "name": "TinyLlama 1.1B Chat GGUF",
      "display_name": "TinyLlama Chat",
      "description": "Small, efficient chat model in GGUF format - perfect for testing LlamaCpp engine",
      "repository_url": "https://huggingface.co",
      "repository_path": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
      "main_filename": "tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
      "file_format": "gguf",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": false,
        "code_interpreter": true,
        "chat": true,
        "text_embedding": false,
        "image_generator": false
      },
      "size_gb": 0.7,
      "tags": ["chat", "small", "efficient", "gguf", "llamacpp", "testing"],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 2048
      },
      "public": true,
      "popularity_score": 0.75,
      "license": "Apache 2.0",
      "quantization_options": [
        {
          "name": "q4_0",
          "main_filename": "tinyllama-1.1b-chat-v1.0.Q4_0.gguf"
        },
        {
          "name": "q4_1",
          "main_filename": "tinyllama-1.1b-chat-v1.0.Q4_1.gguf"
        },
        {
          "name": "q5_0",
          "main_filename": "tinyllama-1.1b-chat-v1.0.Q5_0.gguf"
        },
        {
          "name": "q5_1",
          "main_filename": "tinyllama-1.1b-chat-v1.0.Q5_1.gguf"
        },
        {
          "name": "q8_0",
          "main_filename": "tinyllama-1.1b-chat-v1.0.Q8_0.gguf"
        },
        {
          "name": "fp16",
          "main_filename": "tinyllama-1.1b-chat-v1.0.fp16.gguf"
        }
      ],
      "context_length": 2048,
      "language_support": ["en"],
      "recommended_engine": "llamacpp",
      "recommended_engine_settings": {
        "device_type": "auto",
        "ctx_size": 2048,
        "batch_size": 512,
        "parallel": 1,
        "cont_batching": true,
        "flash_attn": false
      }
    },
    {
      "id": "llama-3-2-3b-instruct-gguf",
      "name": "Llama 3.2 3B Instruct GGUF",
      "display_name": "Llama 3.2 3B",
      "description": "Meta's Llama 3.2 3B instruction-tuned model in GGUF format, optimized for efficiency",
      "repository_url": "https://huggingface.co",
      "repository_path": "bartowski/Llama-3.2-3B-Instruct-GGUF",
      "main_filename": "Llama-3.2-3B-Instruct-Q5_K_S.gguf",
      "file_format": "gguf",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": true,
        "code_interpreter": true,
        "chat": true,
        "text_embedding": false,
        "image_generator": false
      },
      "size_gb": 1.9,
      "tags": [
        "instruct",
        "chat",
        "code",
        "reasoning",
        "gguf",
        "efficient",
        "small"
      ],
      "recommended_parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "context_size": 8192
      },
      "public": true,
      "popularity_score": 0.88,
      "license": "Meta Custom License",
      "quantization_options": [
        { "name": "q4_0", "main_filename": "Llama-3.2-3B-Instruct-Q4_0.gguf" },
        {
          "name": "q4_k_m",
          "main_filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf"
        },
        { "name": "q5_0", "main_filename": "Llama-3.2-3B-Instruct-Q5_0.gguf" },
        {
          "name": "q5_k_m",
          "main_filename": "Llama-3.2-3B-Instruct-Q5_K_M.gguf"
        },
        { "name": "q6_k", "main_filename": "Llama-3.2-3B-Instruct-Q6_K.gguf" },
        { "name": "q8_0", "main_filename": "Llama-3.2-3B-Instruct-Q8_0.gguf" },
        { "name": "fp16", "main_filename": "Llama-3.2-3B-Instruct-FP16.gguf" }
      ],
      "context_length": 8192,
      "language_support": ["en", "es", "fr", "de", "it", "pt", "nl"],
      "recommended_engine": "llamacpp",
      "recommended_engine_settings": {
        "device_type": "auto",
        "ctx_size": 8192,
        "batch_size": 512,
        "parallel": 1,
        "cont_batching": true,
        "flash_attn": false
      }
    },
    {
      "id": "bge-m3",
      "name": "BGE-M3",
      "display_name": "BGE-M3 Multilingual",
      "description": "BAAI's BGE-M3 multilingual embedding model supporting 100+ languages with dense, sparse, and multi-vector retrieval",
      "repository_url": "https://huggingface.co",
      "repository_path": "BAAI/bge-m3",
      "main_filename": "pytorch_model.bin",
      "file_format": "pytorch",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": false,
        "code_interpreter": false,
        "chat": false,
        "text_embedding": true,
        "image_generator": false
      },
      "size_gb": 2.27,
      "tags": [
        "embedding",
        "multilingual",
        "retrieval",
        "rag",
        "text-similarity",
        "dense-retrieval",
        "sparse-retrieval",
        "colbert"
      ],
      "recommended_parameters": {
        "temperature": null,
        "top_p": null,
        "top_k": null,
        "repeat_penalty": null,
        "context_size": 8192
      },
      "public": true,
      "popularity_score": 0.92,
      "license": "MIT",
      "context_length": 8192,
      "language_support": [
        "en",
        "zh",
        "ja",
        "ko",
        "es",
        "fr",
        "de",
        "ru",
        "ar",
        "hi",
        "th",
        "vi",
        "it",
        "pt",
        "nl",
        "pl",
        "tr",
        "fa",
        "id",
        "ms",
        "uk",
        "sv",
        "da",
        "no",
        "fi",
        "cs",
        "sk",
        "hu",
        "ro",
        "bg",
        "hr",
        "sl",
        "et",
        "lv",
        "lt",
        "mt",
        "ga",
        "cy",
        "eu",
        "ca",
        "gl",
        "ast",
        "oc",
        "br",
        "is",
        "fo",
        "kw",
        "gd",
        "gv",
        "lb",
        "rm",
        "fur",
        "sc",
        "co",
        "wa",
        "li",
        "vls",
        "stq",
        "nds",
        "bar",
        "ksh",
        "pdc",
        "hsb",
        "dsb",
        "csb",
        "szl",
        "rue",
        "be",
        "mk",
        "sr",
        "bs",
        "me",
        "sq",
        "el",
        "hy",
        "ka",
        "az",
        "kk",
        "ky",
        "uz",
        "tk",
        "mn",
        "tg",
        "ps",
        "ur",
        "bn",
        "as",
        "or",
        "pa",
        "gu",
        "mr",
        "ne",
        "si",
        "my",
        "km",
        "lo",
        "ka",
        "am",
        "ti",
        "om",
        "so",
        "sw",
        "rw",
        "ny",
        "mg",
        "zu",
        "xh",
        "af",
        "nso",
        "tn",
        "st",
        "ts",
        "ss",
        "ve",
        "nr"
      ],
      "recommended_engine": "mistralrs",
      "recommended_engine_settings": {
        "pooling_mode": "mean",
        "normalize_embeddings": true,
        "dimension": 1024,
        "retrieval_modes": ["dense", "sparse", "colbert"],
        "device_type": "auto"
      }
    },
    {
      "id": "bge-m3-gguf",
      "name": "BGE-M3 GGUF",
      "display_name": "BGE-M3 GGUF Multilingual",
      "description": "BAAI's BGE-M3 multilingual embedding model in GGUF format, optimized for efficient inference with llama.cpp",
      "repository_url": "https://huggingface.co",
      "repository_path": "gpustack/bge-m3-GGUF",
      "main_filename": "bge-m3-Q4_K_M.gguf",
      "file_format": "gguf",
      "capabilities": {
        "vision": false,
        "audio": false,
        "tools": false,
        "code_interpreter": false,
        "chat": false,
        "text_embedding": true,
        "image_generator": false
      },
      "size_gb": 0.44,
      "tags": [
        "embedding",
        "multilingual",
        "retrieval",
        "rag",
        "text-similarity",
        "gguf",
        "efficient",
        "dense-retrieval",
        "sparse-retrieval",
        "colbert"
      ],
      "recommended_parameters": {
        "temperature": null,
        "top_p": null,
        "top_k": null,
        "repeat_penalty": null,
        "context_size": 8192
      },
      "public": true,
      "popularity_score": 0.9,
      "license": "MIT",
      "quantization_options": [
        { "name": "q2_k", "main_filename": "bge-m3-Q2_K.gguf" },
        { "name": "q3_k", "main_filename": "bge-m3-Q3_K.gguf" },
        { "name": "q4_0", "main_filename": "bge-m3-Q4_0.gguf" },
        { "name": "q4_k_m", "main_filename": "bge-m3-Q4_K_M.gguf" },
        { "name": "q5_0", "main_filename": "bge-m3-Q5_0.gguf" },
        { "name": "q5_k_m", "main_filename": "bge-m3-Q5_K_M.gguf" },
        { "name": "q6_k", "main_filename": "bge-m3-Q6_K.gguf" },
        { "name": "q8_0", "main_filename": "bge-m3-Q8_0.gguf" },
        { "name": "fp16", "main_filename": "bge-m3-FP16.gguf" }
      ],
      "context_length": 8192,
      "language_support": [
        "en",
        "zh",
        "ja",
        "ko",
        "es",
        "fr",
        "de",
        "ru",
        "ar",
        "hi",
        "th",
        "vi",
        "it",
        "pt",
        "nl",
        "pl",
        "tr",
        "fa",
        "id",
        "ms",
        "uk",
        "sv",
        "da",
        "no",
        "fi",
        "cs",
        "sk",
        "hu",
        "ro",
        "bg",
        "hr",
        "sl",
        "et",
        "lv",
        "lt",
        "mt",
        "ga",
        "cy",
        "eu",
        "ca",
        "gl",
        "ast",
        "oc",
        "br",
        "is",
        "fo",
        "kw",
        "gd",
        "gv",
        "lb",
        "rm",
        "fur",
        "sc",
        "co",
        "wa",
        "li",
        "vls",
        "stq",
        "nds",
        "bar",
        "ksh",
        "pdc",
        "hsb",
        "dsb",
        "csb",
        "szl",
        "rue",
        "be",
        "mk",
        "sr",
        "bs",
        "me",
        "sq",
        "el",
        "hy",
        "ka",
        "az",
        "kk",
        "ky",
        "uz",
        "tk",
        "mn",
        "tg",
        "ps",
        "ur",
        "bn",
        "as",
        "or",
        "pa",
        "gu",
        "mr",
        "ne",
        "si",
        "my",
        "km",
        "lo",
        "ka",
        "am",
        "ti",
        "om",
        "so",
        "sw",
        "rw",
        "ny",
        "mg",
        "zu",
        "xh",
        "af",
        "nso",
        "tn",
        "st",
        "ts",
        "ss",
        "ve",
        "nr"
      ],
      "recommended_engine": "llamacpp",
      "recommended_engine_settings": {
        "device_type": "auto",
        "ctx_size": 8192,
        "batch_size": 512,
        "parallel": 1,
        "embedding": true,
        "pooling_type": "mean",
        "normalize_embeddings": true,
        "dimension": 1024
      }
    }
  ]
}
